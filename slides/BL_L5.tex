%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,xcolor=svgnames]{beamer}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amsbsy}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

% you can play with different themes and color themes to find your favorite combination.
\mode<presentation> {
  \usetheme{Luebeck}
  \usecolortheme{beaver}
  \beamertemplatenavigationsymbolsempty
  \setbeamertemplate{headline}{}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% include necessary packages here
\usepackage{graphicx} % for including images
\usepackage{pgf} % for logo
\usepackage{colortbl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\date{} % Date, can be changed to a custom date

\titlegraphic{

\includegraphics[width=1.5cm]{/home/mv/Dropbox/IconsAndLogos/LogoBlueJustRing.jpg}\hspace*{2.5cm}~%
\includegraphics[width=2cm]{/home/mv/Dropbox/IconsAndLogos/liulogo.png} \linebreak
\hrulefill \break
\tiny
\includegraphics[width=0.33cm]{/home/mv/Dropbox/IconsAndLogos/web.png} \href{https://mattiasvillani.com}{mattiasvillani.com}\hspace*{1cm}~
\includegraphics[width=0.3cm]{/home/mv/Dropbox/IconsAndLogos/twitter.jpg} \href{https://twitter.com/matvil}{@matvil}\hspace*{1cm}~
\includegraphics[width=0.3cm]{/home/mv/Dropbox/IconsAndLogos/github.png} \href{https://github.com/mattiasvillani}{mattiasvillani}~
}


\definecolor{blue}{RGB}{38, 122, 181}
\definecolor{orange}{RGB}{255, 128, 0}
\definecolor{lorange}{RGB}{255, 178, 102}
\definecolor{llorange}{RGB}{255, 229,204 }
\definecolor{red}{RGB}{255, 128, 0}
\definecolor{verylightgray}{RGB}{246, 246, 246}


\setbeamertemplate{itemize item}{\color{orange}$\blacksquare$}
\setbeamertemplate{itemize subitem}{\color{orange}$\blacktriangleright$}

\usepackage{tcolorbox}

\usepackage[ruled]{algorithm2e}
\usepackage{wasysym}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\makeatother

\usepackage{babel}
\begin{document}
\title[\textcolor{gray}{Multiparameter models}]{\textcolor{orange}{\LARGE{}Bayesian Statistics I}\textcolor{orange}{}}
\subtitle{\textcolor{orange}{Lecture 5 - Large sample approximations. Classification.}}
\author[\textbf{\textcolor{gray}{Mattias Villani}}]{\textbf{Mattias Villani} }
\institute[Linköping and Stockholm University]{Department of Statistics\\
Stockholm University \and Department of Computer and Information
Science\\
Linköping University \medskip{}
}
\makebeamertitle
\begin{frame}{\textbf{\textcolor{orange}{Lecture overview}}}

\begin{itemize}
\item \textbf{\textcolor{blue}{Classification}}\textbf{\textcolor{orange}{\bigskip{}
}}
\item \textbf{\textcolor{blue}{Normal approximation}} of posterior\textbf{\textcolor{orange}{\bigskip{}
}}
\item \textbf{\textcolor{blue}{Logistic regression}} - demo in R
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Bayesian classification}}}

\begin{itemize}
\item \textbf{\textcolor{blue}{Classification: output is a discrete label}}.

\begin{itemize}
\item Binary (0-1). Spam/Ham.
\item Multi-class. ($c=1,2,...,C$). Brand choice.
\end{itemize}
\item \textbf{\textcolor{blue}{Bayesian classification}}
\[
\underset{c\in\mathcal{C}}{\mathrm{argmax}}\:p(c|\mathbf{x})
\]
where $\mathbf{x}=(x_{1},...,x_{p})^{\top}$ is a covariate/feature
vector.
\item \textbf{\textcolor{blue}{Discriminative models}} - model $p(c\vert\mathbf{x})$
directly. 
\begin{itemize}
\item Examples: logistic regression, support vector machines.
\end{itemize}
\item \textbf{\textcolor{blue}{Generative models}} - Use Bayes' theorem
\[
p(c|\mathbf{x})\propto p(\mathbf{x}|c)p(c)
\]
with class-conditional distribution $p(\mathbf{x}\vert c)$ and prior
$p(c)$.
\begin{itemize}
\item Examples: discriminant analysis, naive Bayes.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Naive Bayes}}}

\begin{itemize}
\item By Bayes' theorem
\[
p(c|\mathbf{x})\propto p(\mathbf{x}|c)p(c)
\]
\item $p(c)$ can be estimated by Multinomial-Dirichlet analysis.\medskip{}
\item $p(\mathbf{x}|c)$ can be $N(\theta_{c},\Sigma_{c})$\medskip{}
\item $p(\mathbf{x}|c)$ can be very high-dimensional and hard to estimate.\medskip{}
\item Even with binary features (e.g. \texttt{hasWord('money')} for spam),
the outcome space of \textrm{$p(\mathbf{x}|c)$ can be huge.}\medskip{}
\item \textbf{\textcolor{blue}{Naive Bayes}}: features are assumed\textbf{
}\textbf{\textcolor{blue}{independent}}
\[
p(\mathbf{x}|c)=\prod_{j=1}^{n}p(x_{j}|c)
\]
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Classification with logistic regression}}}

\begin{itemize}
\item Response is assumed to be \textbf{\textcolor{blue}{binary}} ($y=0$
or $1$).
\item Example: Spam/Ham. Covariates: $\$$-symbols, etc.
\item \textbf{\textcolor{blue}{Logistic regression}}
\[
\Pr(y_{i}=1\text{ }|\text{ }x_{i})=\frac{\exp(x_{i}^{\prime}\beta)}{1+\exp(x_{i}^{\prime}\beta)}.
\]
\item \textbf{\textcolor{blue}{Likelihood}}
\[
p(\mathbf{y}|\mathbf{X},\beta)=\prod\nolimits _{i=1}^{n}\frac{[\exp(x_{i}^{\prime}\beta)]^{y_{i}}}{1+\exp(x_{i}^{\prime}\beta)}.
\]
\item Prior $\beta\sim N(0,\tau^{2}I)$. Posterior is non-standard (demo
later).
\item Alternative: \textbf{\textcolor{blue}{Probit regression}}
\[
Pr(y_{i}=1|x_{i})=\Phi(x_{i}^{'}\beta)
\]
\item \textbf{\textcolor{blue}{Multi-class}} ($c=1,2,...,C$) logistic regression
\[
\Pr(y_{i}=c\text{ }|\text{ }x_{i})=\frac{\exp(x_{i}^{\prime}\beta_{c})}{\sum_{k=1}^{C}\exp(x_{i}^{\prime}\beta_{k})}
\]
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Likelihood asymptotics}}}

\begin{itemize}
\item \textbf{\textcolor{blue}{Taylor expansion of log-likelihood}} around
the MLE $\theta=\hat{\theta}$:
\begin{align*}
\ln p(\mathbf{x}|\theta)= & \ln p(\mathbf{x}|\hat{\theta})+\frac{\partial\ln p(\mathbf{x}|\theta)}{\partial\theta}|_{\theta=\hat{\theta}}(\theta-\hat{\theta})\\
 & +\frac{1}{2!}\frac{\partial^{2}\ln p(\mathbf{x}|\theta)}{\partial\theta^{2}}|_{\theta=\hat{\theta}}(\theta-\hat{\theta})^{2}+\dots
\end{align*}
\item Higher order terms ($\ldots$) negligible in large samples.
\item From the definition of the MLE:
\[
\frac{\partial\ln p(\mathbf{x}|\theta)}{\partial\theta}|_{\theta=\hat{\theta}}=0
\]
\item So, in \textbf{\textcolor{blue}{large samples}}
\[
p(\mathbf{x}|\theta)\approx p(\mathbf{x}|\hat{\theta})\exp\left(-\frac{1}{2}J_{\mathbf{x}}(\hat{\theta})(\theta-\hat{\theta})^{2}\right)
\]
\item \textbf{\textcolor{blue}{Observed information}}
\[
J_{\mathbf{x}}(\hat{\theta})=-\frac{\partial^{2}\ln p(\mathbf{x}|\theta)}{\partial\theta^{2}}|_{\theta=\hat{\theta}}
\]
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Likelihood asymptotics}}}
\begin{itemize}
\item $J_{\mathbf{x}}(\hat{\theta})$ varies from sample to sample. \textbf{\textcolor{blue}{Fisher
information}} 
\[
I(\theta)=\mathbb{E}_{\mathbf{x}|\theta}\left(J_{\mathbf{x}}(\hat{\theta})\right)
\]
\item Multiparameter \textbf{\textcolor{blue}{observed information matrix
}}
\[
J_{\boldsymbol{\theta},\mathbf{x}}(\hat{\boldsymbol{\theta}})=-\frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}^{T}}\vert_{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}}
\]
\item Example: \textbf{$\boldsymbol{\theta}=(\theta_{1},\theta_{2})^{\top}$
\[
\frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\boldsymbol{\theta}\partial\boldsymbol{\theta}^{\top}}=\begin{pmatrix}\frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\theta_{1}^{2}} & \frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\theta_{1}\partial\theta_{2}}\\
\frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\theta_{1}\partial\theta_{2}} & \frac{\partial^{2}\ln p(\mathbf{x}\vert\boldsymbol{\theta})}{\partial\theta_{2}^{2}}
\end{pmatrix}.
\]
}
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Posterior asymptotics}}}
\begin{itemize}
\item We can do the same Taylor approximation on log posterior 
\[
\log p(\boldsymbol{\theta}\vert\mathbf{x})=\log p(\mathbf{x}\vert\boldsymbol{\theta})+\log p(\boldsymbol{\theta})-\log p(\boldsymbol{x})
\]
\item \textbf{\textcolor{blue}{Approximate normal posterior}}\textbf{ }in
large samples
\[
\boldsymbol{\theta}|\mathbf{x}\stackrel{\mathrm{approx}}{\sim}N\left[\tilde{\boldsymbol{\theta}},J_{\mathbf{x}}^{-1}(\tilde{\boldsymbol{\theta}})\right]
\]
\item $\tilde{\boldsymbol{\theta}}=\arg\max p(\boldsymbol{\theta}\vert\mathbf{x})$
is the posterior mode and \medskip{}
\item $J_{\mathbf{x}}^{-1}(\tilde{\boldsymbol{\theta}})$ is now with respect
to posterior $\log p(\boldsymbol{\theta}\vert\mathbf{x})$.\medskip{}
\item Likelihood will dominate the prior in large samples so\smallskip{}

\begin{itemize}
\item $\tilde{\boldsymbol{\theta}}\approx\hat{\boldsymbol{\theta}}$ \smallskip{}
\item $J_{\mathbf{x}}^{-1}(\tilde{\boldsymbol{\theta}})$ will be close
to the \textbf{\textcolor{blue}{observed information}}.\smallskip{}
\end{itemize}
\item Important: sufficient with proportional form 
\[
\log p(\boldsymbol{\theta}\vert\mathbf{x})=\log p(\mathbf{x}\vert\boldsymbol{\theta})+\log p(\boldsymbol{\theta})
\]
.
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Example: gamma posterior}}}

\begin{itemize}
\item \textbf{\textcolor{blue}{Poisson model}}: $\theta\vert y_{1},...,y_{n}\sim\mathrm{Gamma}(\alpha+\sum\nolimits _{i=1}^{n}y_{i},\beta+n)$
\[
\log p(\theta|y_{1},...,y_{n})\propto(\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1)\log\theta-\theta(\beta+n)
\]
\item First derivative of log density
\[
\frac{\partial\ln p(\theta|\mathbf{y})}{\partial\theta}=\frac{\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1}{\theta}-(\beta+n)
\]
\[
\tilde{\theta}=\frac{\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1}{\beta+n}
\]
\item Second derivative at mode $\tilde{\theta}$ 
\[
\frac{\partial^{2}\ln p(\theta|\mathbf{y})}{\partial\theta^{2}}|_{\theta=\tilde{\theta}}=-\frac{\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1}{\left(\frac{\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1}{\beta+n}\right)^{2}}=-\frac{(\beta+n)^{2}}{\alpha+\sum\nolimits _{i=\mbox{}1}^{n}y_{i}-1}
\]
\item \textbf{\textcolor{blue}{Normal approximation}}
\[
N\left[\frac{\alpha+\sum\nolimits _{i=1}^{n}y_{i}-1}{\beta+n},\frac{\alpha+\sum\nolimits _{i=1}^{n}y_{i}-1}{(\beta+n)^{2}}\right]
\]
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Example: gamma posterior}}}

\begin{center}
\includegraphics[scale=0.55]{Images/NormalApprox2Gamma}
\par\end{center}

\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Normal approximation of posterior}}}

\begin{itemize}
\item $\boldsymbol{\theta}|\mathbf{y}\stackrel{\mathrm{approx}}{\sim}N\left[\tilde{\boldsymbol{\theta}},J_{\mathbf{y}}^{-1}(\tilde{\boldsymbol{\theta}})\right]$
works also when $\theta$ is a vector.\medskip{}
\item How to compute $\tilde{\boldsymbol{\theta}}$ and $J_{\mathbf{y}}(\tilde{\boldsymbol{\theta}})$?\medskip{}
\item Standard \textbf{\textcolor{blue}{optimization routines}} may be used.
(\texttt{optim.r}). \medskip{}

\begin{itemize}
\item \textbf{\textcolor{blue}{Input}}: expression proportional to $\log p(\theta|\mathbf{y})$.
Initial values.\smallskip{}
\item \textbf{\textcolor{blue}{Output}}: $\log p(\tilde{\boldsymbol{\theta}}|\mathbf{y})$,
$\tilde{\boldsymbol{\theta}}$ and Hessian matrix ($-J_{\mathbf{y}}(\tilde{\boldsymbol{\theta}})$).\medskip{}
\end{itemize}
\item \textbf{\textcolor{blue}{Automatic differentation}} - efficient derivatives
on computer.
\item \textbf{\textcolor{blue}{Re-parametrization}} may improve normal approximation.
{[}Don't forget the \textbf{\textcolor{blue}{Jacobian}}!{]}

\begin{itemize}
\item If $\theta\geq0$ use $\phi=\log(\theta)$. 
\item If $0\leq\theta\leq1$, use $\phi=\ln[\theta/(1-\theta)]$.\medskip{}
\end{itemize}
\item \textbf{\textcolor{blue}{Heavy tailed approximation}}: $\boldsymbol{\theta}|\mathbf{y}\stackrel{\mathrm{approx}}{\sim}t_{v}\left[\tilde{\theta},J_{\mathbf{y}}^{-1}(\tilde{\boldsymbol{\theta}})\right]$
for suitable degrees of freedom $v$.
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Reparametrization - Gamma posterior}}}

\begin{itemize}
\item Poisson model. Reparameterize to $\phi=\log(\theta)$.
\item Change-of-variables formula from a basic probability course
\[
\log p(\phi|y_{1},...,y_{n})\propto(\alpha+\sum\nolimits _{i=1}^{n}y_{i}-1)\phi-\exp(\phi)(\beta+n)+\phi
\]
\item Taking first and second derivatives and evaluating at $\tilde{\phi}$
gives
\[
\tilde{\phi}=\log\left(\frac{\alpha+\sum\nolimits _{i=1}^{n}y_{i}}{\beta+n}\right)\text{ and }\frac{\partial^{2}\ln p(\phi|y)}{\partial\phi^{2}}|_{\phi=\tilde{\phi}}=\alpha+\sum\nolimits _{i=1}^{n}y_{i}
\]
\item So, the normal approximation for $p(\phi\vert y_{1},...y_{n})$ is
\[
\phi=\log(\theta)\sim N\left[\log\left(\frac{\alpha+\sum\nolimits _{i=1}^{n}y_{i}}{\beta+n}\right),\frac{1}{\alpha+\sum\nolimits _{i=1}^{n}y_{i}}\right]
\]
which means that $p(\theta\vert y_{1},...y_{n})$ is log-normal: 
\[
\theta\vert\mathbf{y}\sim LN\left[\log\left(\frac{\alpha+\sum\nolimits _{i=1}^{n}y_{i}}{\beta+n}\right),\frac{1}{\alpha+\sum\nolimits _{i=1}^{n}y_{i}}\right]
\]
\end{itemize}
\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Reparametrization - Gamma posterior}}}

\begin{center}
\includegraphics[scale=0.55]{Images/NormalApprox2GammaTrans}
\par\end{center}

\end{frame}

\begin{frame}{\textbf{\textcolor{orange}{Normal approximation of posterior}}}

\begin{itemize}
\item Even if the posterior of $\theta$ is approx normal, \textbf{\textcolor{blue}{interesting
functions}} of $g(\theta)$ may not be (e.g. predictions). \bigskip{}
\item But approximate posterior of $g(\theta)$ can be obtained by \textbf{\textcolor{blue}{simulating}}
from $N\left[\tilde{\theta},J_{\mathbf{y}}^{-1}(\tilde{\theta})\right]$.\bigskip{}
\item Posterior of \textbf{\textcolor{blue}{Gini coefficient}}

\begin{itemize}
\item Model: $x_{1},...,x_{n}\vert\mu,\sigma^{2}\sim LN(\mu,\sigma^{2})$. 
\item Let $\phi=\log(\sigma^{2})$. And $\boldsymbol{\theta}=(\mu,\phi)$.
\item Joint posterior $p(\mu,\phi)$ may be approximately normal: $\boldsymbol{\theta}|\mathbf{y}\stackrel{\mathrm{approx}}{\sim}N\left[\tilde{\boldsymbol{\theta}},J_{\mathbf{y}}^{-1}(\tilde{\boldsymbol{\theta}})\right]$.
\item Simulate $\boldsymbol{\theta}^{(1)},...,\boldsymbol{\theta}^{(N)}$
from $N\left[\tilde{\boldsymbol{\theta}},J_{\mathbf{y}}^{-1}(\tilde{\boldsymbol{\theta}})\right]$. 
\item Compute $\sigma^{(1)},...,\sigma^{(N)}$.
\item Compute $G^{(i)}=2\Phi\left(\sigma^{(i)}/\sqrt{2}\right)$ for $i=1,...,N$.
\end{itemize}
\end{itemize}
\end{frame}


\end{document}
